library(ROCR)
library(tree)
library(class)
train = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Train.csv')
x.test = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Xtest.csv')
Ans = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Ans.csv')
#train데이터를 train, test데이터로 나누기
set.seed(20130624)
n = 4000
train_num = sample(n, n/2)
train_1 = train[train_num,]
test_1 = train[-train_num,]
train_1 = train_1[,-1] #x변수 제거
test_1 = test_1[,-1] #x변수 제거
#5. classification tree classifier
fit_tree = tree(y~., data = train_1)
#prune
fit_prune = cv.tree(fit_tree, FUN = prune.misclass)
fit_prune
#5. classification tree classifier
fit_tree = tree(y~., data = train_1)
fit_tree
#prune
fit_prune = cv.tree(fit_tree, FUN = prune.misclass)
train_x = train_1[,-51]
train_y = train_1[,51]
test_x = test_1[,-51]
test_y = test_1[,51]
train_x = data.matrix(train_x)
test_x = data.matrix(test_x)
fit_xgb = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 10,
max.depth = 5,
nrounds = 120,
verbose = F)
pred_xgb = predict(fit_xgb, test_x)
pred_xgb = pred_xgb-1
confusionMatrix(pred_xgb, as.factor(test_1$y), positive = '0', mode = 'everything')
str(pred_xgb)
pred_xgb = predict(fit_xgb, test_x)
pred_xgb = pred_xgb
str(pred_xgb)
mean(pred_xgb != test_y)
confusionMatrix(pred_xgb, as.factor(test_1$y), positive = '0', mode = 'everything')
confusionMatrix(as.factor(pred_xgb), as.factor(test_1$y), positive = '0', mode = 'everything')
test_x = x.test
str(test_x)
train_x = data.matrix(train_x)
test_x = data.matrix(test_x)
fit_xgb_test = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 10,
max.depth = 5,
nrounds = 120,
verbose = F)
pred_xgb_test = predict(fit_xgb_test, test_x)
fit_x
fit_xgb_test
pred_xgb_test = predict(fit_xgb_test, test_x)
train_x = train[,-51]
train_y = train[,51]
fit_xgb_test = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 10,
max.depth = 5,
nrounds = 120,
verbose = F)
train_x = data.matrix(train_x)
test_x = data.matrix(test_x)
fit_xgb_test = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 10,
max.depth = 5,
nrounds = 120,
verbose = F)
pred_xgb_test = predict(fit_xgb_test, test_x)
fit_xgb_test
test_x = data.matrix(test_x)
pred_xgb_test = predict(fit_xgb_test, test_x)
str(test_x)
test_x = x.test
str(test_x)
test_x = test_x[,-1]
str(test_x)
test_x = data.matrix(test_x)
fit_xgb_test = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 10,
max.depth = 5,
nrounds = 120,
verbose = F)
pred_xgb_test = predict(fit_xgb_test, test_x)
fit_xgb_test
#Accuracy or AUC 30점
#Methods and program 평가는
#1) 얼마나 많은 방법을 사용했는지
#2) 문제에 적절하게 적용됐는지
#3) 에러없이 작동했는지
#Accuracy or AUC 평가는
#Ans.csv 파일을 바탕으로 [(Accuracy)-85] X 2 해서 85미만이면 0점
############################################################################
############################################################################
#데이터 불러오기
train = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Train.csv')
x.test = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Xtest.csv')
Ans = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Ans.csv')
############################################################################
############################################################################
#xtest.csv에 knn(k = 3)일 때의 모델로 y predict
train = train[,-1]
x.test = x.test[,-1]
str(train)
str(x.test)
#xgboost로 y predict
train_x = train[,-51]
train_y = train[,51]
test_x = x.test
str(test_x)
train_x = data.matrix(train_x)
test_x = data.matrix(test_x)
fit_xgb_test = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 10,
max.depth = 5,
nrounds = 120,
verbose = F)
pred_xgb_test = predict(fit_xgb_test, test_x)
pred_xgb_test
############################################################################
#X.test 로 predict한 결과 Ans.csv에 입력
str(Ans)
Ans$yhat = pred_xgb_test
############################################################################
#X.test 로 predict한 결과 Ans.csv에 입력
str(Ans)
pred_xgb_prob = predict_proba(fit_xgb_test, test_x)
#X.test 로 predict하여 나온 posterior probability Ans.csv에 입력
fit_xgb_prob = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softprob',
num_class = 10,
max.depth = 5,
nrounds = 120,
verbose = F)
fit_xgb_prob
pred_xgb_prob = predict(fit_xgb_prob, test_x)
pred_xgb_prob
str(pred_xgb_prob)
#Accuracy or AUC 30점
#Methods and program 평가는
#1) 얼마나 많은 방법을 사용했는지
#2) 문제에 적절하게 적용됐는지
#3) 에러없이 작동했는지
#Accuracy or AUC 평가는
#Ans.csv 파일을 바탕으로 [(Accuracy)-85] X 2 해서 85미만이면 0점
############################################################################
############################################################################
#데이터 불러오기
train = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Train.csv')
train_x = train_1[,-51]
train_y = train_1[,51]
test_x = test_1[,-51]
test_y = test_1[,51]
train_x = data.matrix(train_x)
test_x = data.matrix(test_x)
fit_xgb = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 5,
nrounds = 120,
verbose = F)
pred_xgb = predict(fit_xgb, test_x)
confusionMatrix(as.factor(pred_xgb), as.factor(test_1$y), positive = '0', mode = 'everything')
mean(pred_xgb != test_y)
train = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Train.csv')
x.test = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Xtest.csv')
Ans = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Ans.csv')
train = train[,-1]
x.test = x.test[,-1]
#xgboost로 y predict
train_x = train[,-51]
train_y = train[,51]
test_x = x.test
train_x = data.matrix(train_x)
test_x = data.matrix(test_x)
fit_xgb_test = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 5,
nrounds = 120,
verbose = F)
pred_xgb_test = predict(fit_xgb_test, test_x)
pred_xgb_test
Ans$yhat = pred_xgb_test
############################################################################
#X.test 로 predict한 결과 Ans.csv에 입력
str(Ans)
#X.test 로 predict하여 나온 posterior probability Ans.csv에 입력
fit_xgb_prob = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softprob',
num_class = 2,
max.depth = 5,
nrounds = 120,
verbose = F)
pred_xgb_prob = predict(fit_xgb_prob, test_x)
str(pred_xgb_prob)
list(pred_xgb_prob)
(pred_xgb_prob)
result(pred_xgb_prob)
summary(pred_xgb_prob)
#X.test 로 predict하여 나온 posterior probability Ans.csv에 입력
fit_xgb_prob = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softprob',
num_class = 1,
max.depth = 5,
nrounds = 120,
verbose = F)
2
#X.test 로 predict하여 나온 posterior probability Ans.csv에 입력
fit_xgb_prob = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softprob',
num_class = 2,
max.depth = 5,
nrounds = 120,
verbose = F)
#X.test 로 predict하여 나온 posterior probability Ans.csv에 입력
fit_xgb_prob = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softprob',
num_class = 2,
max.depth = 5,
nrounds = 120,
verbose = T)
pred_xgb_prob = predict(fit_xgb_prob, test_x)
str(pred_xgb_prob)
pred_xgb_prob
pred_xgb_test = predict(fit_xgb_test, test_x)
pred_xgb_test
#Accuracy or AUC 30점
#Methods and program 평가는
#1) 얼마나 많은 방법을 사용했는지
#2) 문제에 적절하게 적용됐는지
#3) 에러없이 작동했는지
#Accuracy or AUC 평가는
#Ans.csv 파일을 바탕으로 [(Accuracy)-85] X 2 해서 85미만이면 0점
############################################################################
############################################################################
#데이터 불러오기
train = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Train.csv')
train_1 = train_1[,-1] #x변수 제거
test_1 = test_1[,-1] #x변수 제거
train_x = train_1[,-51]
train_y = train_1[,51]
test_x = test_1[,-51]
test_y = test_1[,51]
#Accuracy or AUC 30점
#Methods and program 평가는
#1) 얼마나 많은 방법을 사용했는지
#2) 문제에 적절하게 적용됐는지
#3) 에러없이 작동했는지
#Accuracy or AUC 평가는
#Ans.csv 파일을 바탕으로 [(Accuracy)-85] X 2 해서 85미만이면 0점
############################################################################
############################################################################
#데이터 불러오기
train = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Train.csv')
set.seed(20130624)
n = 4000
train_num = sample(n, n/2)
train_1 = train[train_num,]
test_1 = train[-train_num,]
train_1 = train_1[,-1] #x변수 제거
test_1 = test_1[,-1] #x변수 제거
train_x = train_1[,-51]
train_y = train_1[,51]
test_x = test_1[,-51]
test_y = test_1[,51]
#7-1. SVC
tune.svc = tune(method = 'svm', train.x = as.matrix(model.matrix(~.-1, data = train_x)),
train.y = train_y, kernel = 'linear',
ranges = list(cost = c(0.01, 0.1, 1)), best.model = TRUE)
tune.svc
fit_svc = tune.svc$best.model
pred_svc = predict(fit_svc, as.matrix(model.matrix(~.-1, data = test_x)))
confusionMatrix(pred_svc, test_y)
str(pred_svc)
str(test_y)
confusionMatrix(pred_svc, as.factor(test_y))
confusionMatrix(pred_svc, test_y)
pred_svc
confusionMatrix(pred_svc, test_y)
str(test_y)
str(test_x)
str(train_x)
str(train_y)
fit_svc = tune.svc$best.model
pred_svc = predict(fit_svc, as.matrix(model.matrix(~.-1, data = test_x)))
confusionMatrix(pred_svc, test_y)
str(pred_svc)
tune.svc
pred_svc = predict(fit_svc, as.matrix(model.matrix(~., data = test_x)))
pred_svc = predict(fit_svc, as.matrix(model.matrix(test_x)))
str(test_x)
pred_svc = predict(fit_svc, as.matrix(test_x))
str(pred_svc)
confusionMatrix(pred_svc, test_y)
#7-1. SVC
tune.svc = tune(method = 'svm', train.x = as.matrix(train_x),
train.y = train_y, kernel = 'linear',
ranges = list(cost = c(0.01, 0.1, 1)), best.model = TRUE)
train_y = as.factor(train_y)
str(train_y)
#7-1. SVC
tune.svc = tune(method = 'svm', train.x = as.matrix(train_x),
train.y = train_y, kernel = 'linear',
ranges = list(cost = c(0.01, 0.1, 1)), best.model = TRUE)
tune.svc
fit_svc = tune.svc$best.model
pred_svc = predict(fit_svc, as.matrix(test_x))
confusionMatrix(pred_svc, test_y)
str(test_x)
str(pred_svc)
pred_svc
confusionMatrix(pred_svc, test_y)
test_y = as.factor(test_y)
confusionMatrix(pred_svc, test_y)
#7-1. SVC
tune.svc = tune(method = 'svm', train.x = as.matrix(train_x),
train.y = train_y, kernel = 'linear',
ranges = list(cost = c(0.01, 0.1, 0.5, 1, 5, 10)), best.model = TRUE)
tune.svc
fit_svc = tune.svc$best.model
pred_svc = predict(fit_svc, as.matrix(test_x))
confusionMatrix(pred_svc, test_y)
#7-3. SVMR
tune.svmr = tune(method = 'svm', train.x = as.matrix(train_x),
train.y = train_y, kernel = 'radial',
ranges = list(cost = c(0.1, 0.5, 1, 5, 10),gamma = c(1, 2, 3)),
best.model = TRUE)
tune.svmr
fit_svmr = tune.svmr$best.model
pred_svmr = predict(fit_svmr, as.matrix(test_x))
confusionMatrix(pred_svmr, test_y)
fit_svc
pred_svc
list(pred_svc)
confusionMatrix(pred_svc, test_y, positive = '1', mode = 'everything')
cm = confusionMatrix(pred_svc, test_y, positive = '1', mode = 'everything')
cm$Pos Pred Value
str(cm)
cm$overall
cm$byClass
fit_svc
str(pred_svc)
confusionMatrix(pred_svc, test_y, positive = '1', mode = 'everything')
pred_xgb_prob
str(test_x)
test_x = x.test
str(test_x)
pred_xgb_prob
pred_xgb_prob_1 = pred_xgb_prob[,2]>0.5
pred_xgb_prob_1 = pred_xgb_prob[,1]>0.5
pred_xgb_prob
str(pred_xgb_prob)
pred_xgb_prob_1 = pred_xgb_prob[1,]>0.5
pred_xgb_prob_1 = pred_xgb_prob[>0.5,]
pred_xgb_prob_1 = pred_xgb_prob[[,1]>0.5,]
pred_xgb_prob_1 = subset[pred_xgb_prob>0.5]
pred_xgb_prob_1 = subset[pred_xgb_prob, >0.5]
df = data.frame(pred_xgb_prob)
df
prob = subset(df, pred_xgb_prob>0.5)
prob
str(prob)
as.numeric(prob)
prob1 = as.numeric(prob$pred_xgb_prob)
prob1
prob1 = prob$pred_xgb_prob
prob1
Ans$prob = prob1
str(Ans)
write.csv(Ans,''C:/T/과제/5학년 1학기/데이터마이닝/final/data/Ans1.csv'')
write.csv(Ans,'C:/T/과제/5학년 1학기/데이터마이닝/final/data/Ans1.csv')
library(ISLR)
library(caret)
library(gbm)
library(randomForest)
library(ranger)
library(xgboost)
library(dplyr)
library(e1071)
library(ROCR)
library(tree)
library(class)
############################################################################
############################################################################
#Ans.csv 채우기
train = read.csv('C:/T/Train.csv')
x.test = read.csv('C:/T/Xtest.csv')
############################################################################
############################################################################
#Ans.csv 채우기
train = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final')
x.test = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/Xtest.csv')
############################################################################
############################################################################
#Ans.csv 채우기
train = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/data/final')
############################################################################
############################################################################
#Ans.csv 채우기
train = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/data/Train.csv')
x.test = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Xtest.csv')
############################################################################
############################################################################
#Ans.csv 채우기
train = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Train.csv')
Ans = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Ans.csv')
train = train[,-1]
x.test = x.test[,-1]
train_x = train[,-51]
train_y = train[,51]
test_x = x.test
train_x = data.matrix(train_x)
test_x = data.matrix(test_x)
library(ISLR)
library(caret)
library(gbm)
library(randomForest)
library(ranger)
library(xgboost)
library(dplyr)
library(e1071)
library(ROCR)
library(tree)
library(class)
############################################################################
############################################################################
#Ans.csv 채우기
train = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Train.csv')
x.test = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Xtest.csv')
Ans = read.csv('C:/T/과제/5학년 1학기/데이터마이닝/final/data/Ans.csv')
train = train[,-1]
x.test = x.test[,-1]
train_x = train[,-51]
train_y = train[,51]
test_x = x.test
train_x = data.matrix(train_x)
test_x = data.matrix(test_x)
fit_xgb_test = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softmax',
num_class = 2,
max.depth = 5,
nrounds = 120,
verbose = F)
pred_xgb_test = predict(fit_xgb_test, test_x)
pred_xgb_test
#X.test 로 predict한 결과 Ans.csv에 입력
Ans$yhat = pred_xgb_test
#X.test 로 predict하여 나온 posterior probability Ans.csv에 입력
fit_xgb_prob = xgboost(train_x, as.numeric(train_y),
booster = 'gbtree',
objective = 'multi:softprob',
num_class = 2,
max.depth = 5,
nrounds = 120,
verbose = F)
hist(pred_xgb_test)
sum(pred_xgb_test
sum(pred_xgb_test)
sum(pred_xgb_test)
setwd('C:/T/08 repository/project-gangseo/data')
data = read.csv('3차완료.csv')
data
head(data)
col(data)
type(data)
df
data = read.csv('3차완료.csv')
head(data)
df = subset(data, select = -target)
df
colnames(df)
summary(fit)
fit = lm(accident ~ ., data = df)
summary(fit)
fit
df = subset(data, select = -(target, crosswalk_id))
df = subset(df, select = -crosswalk_id)
colnames(df)
fit = lm(accident ~ ., data = df)
fit
summary(fit)
step(fit, direction = 'both')
colnames(df)
fit1 = lm(accident ~ intersection + trafficlight + school +
illegal_parking + cctv + traffic_sign +
subway_pop + bus_pop + cafe_100, data = df)
summary(fit1)
